{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f348507b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EnergyNexus Advanced LSTM Architectures - OPTIMIZED FOR 85%+ ACCURACY\n",
      "======================================================================\n",
      "Development started: 2025-08-08 01:35:29\n",
      "TensorFlow version: 2.19.0\n",
      "\n",
      "EXECUTING OPTIMIZED ADVANCED LSTM PIPELINE FOR 85%+ ACCURACY\n",
      "======================================================================\n",
      "\n",
      "STEP 1: ENHANCED DATA LOADING AND PREPARATION\n",
      "--------------------------------------------------\n",
      "Successfully loaded processed energy dataset\n",
      "Adding advanced features for enhanced LSTM performance...\n",
      "Enhanced dataset shape after feature engineering: (201604, 64)\n",
      "Data source: Processed pipeline data\n",
      "Dataset shape: (201604, 64)\n",
      "Date range: 2014-12-31 23:00:00+00:00 to 2020-09-30 23:45:00+00:00\n",
      "Data completeness: 100.0%\n",
      "Dataset size: 201604 records\n",
      "Large dataset detected - moderate sampling to preserve patterns...\n",
      "Sampled dataset size: 33601 records (sample rate: 1/6)\n",
      "ENHANCED PARAMETERS for 85%+ accuracy:\n",
      "  Sequence length: 168 hours (WEEKLY PATTERNS)\n",
      "  Forecast horizons: [1, 6, 24]\n",
      "Available columns: 64 total\n",
      "Using first 3 numeric columns as targets: ['AT_load_actual_entsoe_transparency', 'AT_load_forecast_entsoe_transparency', 'AT_price_day_ahead']\n",
      "Selected 48 ENHANCED features for higher accuracy\n",
      "Selected targets: ['AT_load_actual_entsoe_transparency', 'AT_load_forecast_entsoe_transparency', 'AT_price_day_ahead']\n",
      "Selected features: 48 features\n",
      "\n",
      "STEP 2: ENHANCED SEQUENCE CREATION\n",
      "----------------------------------------\n",
      "Creating multi-variate sequences for advanced LSTM modeling\n",
      "Target variables: ['AT_load_actual_entsoe_transparency', 'AT_load_forecast_entsoe_transparency', 'AT_price_day_ahead']\n",
      "Feature variables: 48 features\n",
      "Sequence length: 168 hours (EXTENDED FOR WEEKLY PATTERNS)\n",
      "Forecast horizons: [1, 6, 24]\n",
      "Created multi-variate sequences:\n",
      "  X_shape: (33409, 168, 48)\n",
      "  y_shape: (33409, 9)\n",
      "  Output structure: 3 targets × 3 horizons\n",
      "Memory required: 2.01 GB\n",
      "\n",
      "STEP 3: ENHANCED DATA SPLITTING\n",
      "-----------------------------------\n",
      "ENHANCED splits for 85%+ accuracy:\n",
      "  Training: 25056 sequences (75%)\n",
      "  Validation: 5011 sequences (15%)\n",
      "  Test: 3342 sequences (10%)\n",
      "\n",
      "STEP 4: ENHANCED DATA NORMALIZATION\n",
      "----------------------------------------\n",
      "Enhanced normalization completed\n",
      "  Feature scaling: RobustScaler (better outlier handling)\n",
      "  Target scaling: StandardScaler\n",
      "  Data variance for R² estimation: 0.672800\n",
      "\n",
      "STEP 5: BUILDING ENHANCED MODEL ARCHITECTURES\n",
      "--------------------------------------------------\n",
      "Enhanced model configuration:\n",
      "  Input shape: (168, 48)\n",
      "  Output size: 9\n",
      "  Architecture focus: DEEP networks for 85%+ accuracy\n",
      "Building SUPER ENSEMBLE with 3 diverse models for 85%+ accuracy\n",
      "  Building ensemble member 1: deep_attention_lstm\n",
      "Building DEEP attention-based LSTM model for 85%+ accuracy\n",
      "Input shape: (168, 48)\n",
      "Output size: 9\n",
      "Architecture: DEEPER with 3 LSTM layers\n",
      "  DEEP LSTM Layer 1: 128 units\n",
      "  DEEP LSTM Layer 2: 64 units\n",
      "  DEEP LSTM Layer 3: 32 units\n",
      "  DEEP Attention-based LSTM model compiled successfully\n",
      "  Total parameters: 171,146\n",
      "    ✓ deep_attention_lstm built successfully\n",
      "  Building ensemble member 2: hybrid_cnn_lstm\n",
      "Building HYBRID CNN-LSTM model for enhanced feature extraction\n",
      "  CNN Layer 1: 64 filters\n",
      "  CNN Layer 2: 32 filters\n",
      "  LSTM Layer 1: 128 units\n",
      "  LSTM Layer 2: 64 units\n",
      "  Hybrid CNN-LSTM model compiled successfully\n",
      "  Total parameters: 165,865\n",
      "    ✓ hybrid_cnn_lstm built successfully\n",
      "  Building ensemble member 3: transformer_lstm\n",
      "Building TRANSFORMER-enhanced LSTM model\n",
      "  LSTM Layer 1: 128 units\n",
      "  LSTM Layer 2: 64 units\n",
      "  Transformer-LSTM model compiled successfully\n",
      "  Total parameters: 307,977\n",
      "    ✓ transformer_lstm built successfully\n",
      "  SUPER ENSEMBLE built with 3 diverse models\n",
      "Enhanced models built successfully\n",
      "\n",
      "STEP 6: ENHANCED MODEL TRAINING FOR 85%+ ACCURACY\n",
      "-------------------------------------------------------\n",
      "ENHANCED TRAINING CONFIGURATION:\n",
      "  Epochs: 150 (vs previous 20)\n",
      "  Batch size: 16 (smaller for better gradients)\n",
      "  Advanced callbacks: Extended patience, cyclical LR\n",
      "\n",
      "--- Training Enhanced Model 1: deep_attention_lstm ---\n",
      "Training ADVANCED deep_attention_lstm model for 85%+ accuracy...\n",
      "  Training samples: 25056\n",
      "  Validation samples: 5011\n",
      "  EXTENDED TRAINING: 150 epochs (vs previous 20)\n",
      "  SMALLER BATCH SIZE: 16 (for better gradients)\n",
      "Setting up ADVANCED callbacks for deep_attention_lstm (targeting 85%+ accuracy)...\n",
      "  ADVANCED callbacks configured: 5 callbacks\n",
      "  Early stopping patience: 50 epochs\n",
      "  Advanced learning rate scheduling enabled\n",
      "Epoch 1/150\n",
      "\u001b[1m 165/1566\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8:30\u001b[0m 364ms/step - loss: 1.3651 - mae: 0.9278 - mape: 396.9578"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1166\u001b[0m\n\u001b[0;32m   1164\u001b[0m \u001b[38;5;66;03m# Execute the optimized pipeline\u001b[39;00m\n\u001b[0;32m   1165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1166\u001b[0m     best_model, results \u001b[38;5;241m=\u001b[39m main_optimized_for_accuracy()\n",
      "Cell \u001b[1;32mIn[1], line 1044\u001b[0m, in \u001b[0;36mmain_optimized_for_accuracy\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1040\u001b[0m model \u001b[38;5;241m=\u001b[39m model_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Training Enhanced Model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1044\u001b[0m history \u001b[38;5;241m=\u001b[39m train_advanced_model_for_accuracy(\n\u001b[0;32m   1045\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   1046\u001b[0m     model_name\u001b[38;5;241m=\u001b[39mmodel_name,\n\u001b[0;32m   1047\u001b[0m     X_train\u001b[38;5;241m=\u001b[39mX_train_scaled,\n\u001b[0;32m   1048\u001b[0m     y_train\u001b[38;5;241m=\u001b[39my_train_scaled,\n\u001b[0;32m   1049\u001b[0m     X_val\u001b[38;5;241m=\u001b[39mX_val_scaled,\n\u001b[0;32m   1050\u001b[0m     y_val\u001b[38;5;241m=\u001b[39my_val_scaled,\n\u001b[0;32m   1051\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mENHANCED_EPOCHS,\n\u001b[0;32m   1052\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mENHANCED_BATCH_SIZE\n\u001b[0;32m   1053\u001b[0m )\n\u001b[0;32m   1055\u001b[0m training_results[model_name] \u001b[38;5;241m=\u001b[39m history\n\u001b[0;32m   1057\u001b[0m \u001b[38;5;66;03m# Estimate R² from validation loss\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 641\u001b[0m, in \u001b[0;36mtrain_advanced_model_for_accuracy\u001b[1;34m(model, model_name, X_train, y_train, X_val, y_val, epochs, batch_size)\u001b[0m\n\u001b[0;32m    637\u001b[0m training_start \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    640\u001b[0m     \u001b[38;5;66;03m# Train model with extended epochs\u001b[39;00m\n\u001b[1;32m--> 641\u001b[0m     history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m    642\u001b[0m         X_train, y_train,\n\u001b[0;32m    643\u001b[0m         epochs\u001b[38;5;241m=\u001b[39mepochs,  \u001b[38;5;66;03m# Extended training\u001b[39;00m\n\u001b[0;32m    644\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,  \u001b[38;5;66;03m# Smaller batch size\u001b[39;00m\n\u001b[0;32m    645\u001b[0m         validation_data\u001b[38;5;241m=\u001b[39m(X_val, y_val),\n\u001b[0;32m    646\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39madvanced_callbacks,\n\u001b[0;32m    647\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    648\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# Maintain temporal order\u001b[39;00m\n\u001b[0;32m    649\u001b[0m     )\n\u001b[0;32m    651\u001b[0m     training_end \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m    652\u001b[0m     training_duration \u001b[38;5;241m=\u001b[39m training_end \u001b[38;5;241m-\u001b[39m training_start\n",
      "File \u001b[1;32mc:\\Users\\ADITYA\\anaconda3\\envs\\PyAgrum\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\ADITYA\\anaconda3\\envs\\PyAgrum\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:377\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[0;32m    376\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 377\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m    378\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[1;32mc:\\Users\\ADITYA\\anaconda3\\envs\\PyAgrum\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:220\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    218\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[0;32m    219\u001b[0m     ):\n\u001b[1;32m--> 220\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m multi_step_on_iterator(iterator)\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[0;32m    222\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ADITYA\\anaconda3\\envs\\PyAgrum\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\ADITYA\\anaconda3\\envs\\PyAgrum\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\ADITYA\\anaconda3\\envs\\PyAgrum\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    879\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[0;32m    880\u001b[0m )\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ADITYA\\anaconda3\\envs\\PyAgrum\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ADITYA\\anaconda3\\envs\\PyAgrum\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\ADITYA\\anaconda3\\envs\\PyAgrum\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\ADITYA\\anaconda3\\envs\\PyAgrum\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ADITYA\\anaconda3\\envs\\PyAgrum\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1686\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1688\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[0;32m   1689\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1690\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   1691\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[0;32m   1692\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[0;32m   1693\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1694\u001b[0m   )\n\u001b[0;32m   1695\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1696\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1697\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1698\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1702\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1703\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\ADITYA\\anaconda3\\envs\\PyAgrum\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "EnergyNexus Advanced LSTM Architectures - OPTIMIZED FOR 85%+ ACCURACY\n",
    "Aditya's MSc Project - Enhanced LSTM Models for Multi-variate Energy Forecasting\n",
    "\n",
    "Key Optimizations:\n",
    "1. Extended training epochs (100+ vs 20)\n",
    "2. Advanced feature engineering\n",
    "3. Deeper LSTM architectures\n",
    "4. Better learning rate scheduling\n",
    "5. Enhanced data preprocessing\n",
    "6. Cyclical learning rates\n",
    "7. Advanced ensemble methods\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Suppress warnings for clean output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"Set2\")\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Add source directory\n",
    "sys.path.append(os.path.join('..', '..', 'src'))\n",
    "\n",
    "print(\"EnergyNexus Advanced LSTM Architectures - OPTIMIZED FOR 85%+ ACCURACY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Development started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Import libraries with error handling\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers, Model, Input, callbacks, optimizers\n",
    "    from tensorflow.keras.layers import (LSTM, Dense, Dropout, BatchNormalization, \n",
    "                                        Attention, MultiHeadAttention, LayerNormalization,\n",
    "                                        Bidirectional, TimeDistributed, RepeatVector, \n",
    "                                        GRU, Conv1D, GlobalMaxPooling1D, Concatenate)\n",
    "    from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "    from sklearn.model_selection import TimeSeriesSplit\n",
    "    \n",
    "    print(f\"TensorFlow version: {tf.__version__}\")\n",
    "    tf.random.set_seed(42)\n",
    "    TENSORFLOW_AVAILABLE = True\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"Advanced libraries not available: {e}\")\n",
    "    TENSORFLOW_AVAILABLE = False\n",
    "\n",
    "# =============================================================================\n",
    "# ENHANCED DATA PREPARATION WITH ADVANCED FEATURE ENGINEERING\n",
    "# =============================================================================\n",
    "\n",
    "def load_comprehensive_energy_data():\n",
    "    \"\"\"Load and prepare comprehensive energy dataset with enhanced features.\"\"\"\n",
    "    try:\n",
    "        energy_data = pd.read_csv('../../data/processed/test_cleaned_energy_data.csv', \n",
    "                                 parse_dates=[0], index_col=0)\n",
    "        print(\"Successfully loaded processed energy dataset\")\n",
    "        data_source = \"Processed pipeline data\"\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"Creating enhanced energy dataset for advanced modeling...\")\n",
    "        \n",
    "        # Generate realistic multi-variate energy data\n",
    "        np.random.seed(42)\n",
    "        hours = 24 * 180  # 180 days for better patterns\n",
    "        dates = pd.date_range(start='2024-01-01', periods=hours, freq='H')\n",
    "        time_hours = np.arange(hours)\n",
    "        \n",
    "        # Enhanced weather patterns with more realism\n",
    "        base_temperature = (15 + \n",
    "                           12 * np.sin(2 * np.pi * dates.dayofyear / 365) +\n",
    "                           8 * np.sin((time_hours % 24 - 14) * 2 * np.pi / 24) +\n",
    "                           3 * np.sin(2 * np.pi * dates.dayofyear / 7) +  # Weekly variation\n",
    "                           np.random.normal(0, 2, hours))\n",
    "        \n",
    "        wind_speed = (8 + \n",
    "                     4 * np.sin(2 * np.pi * dates.dayofyear / 365 + np.pi/3) +\n",
    "                     2 * np.sin(2 * np.pi * time_hours / (24*7)) +  # Weekly pattern\n",
    "                     np.random.normal(0, 2, hours))\n",
    "        wind_speed = np.maximum(0, wind_speed)\n",
    "        \n",
    "        cloud_cover = np.random.beta(2, 5, hours) * 100\n",
    "        \n",
    "        # Enhanced solar generation with better modeling\n",
    "        solar_elevation = np.maximum(0, np.sin((time_hours % 24 - 12) * np.pi / 12))\n",
    "        seasonal_solar = 1 + 0.3 * np.sin(2 * np.pi * dates.dayofyear / 365)\n",
    "        cloud_attenuation = 1 - (cloud_cover / 100) * 0.8\n",
    "        temperature_efficiency = 1 - np.maximum(0, base_temperature - 25) * 0.004\n",
    "        \n",
    "        solar_generation = (solar_elevation * seasonal_solar * cloud_attenuation * \n",
    "                           temperature_efficiency * 250 + np.random.normal(0, 8, hours))\n",
    "        solar_generation = np.maximum(0, solar_generation)\n",
    "        \n",
    "        # Enhanced wind generation with realistic power curve\n",
    "        wind_generation = np.zeros(hours)\n",
    "        for i, ws in enumerate(wind_speed):\n",
    "            if ws < 3:  # Cut-in speed\n",
    "                wind_generation[i] = 0\n",
    "            elif ws < 12:  # Cubic region\n",
    "                wind_generation[i] = 150 * ((ws - 3) / 9) ** 3\n",
    "            elif ws < 25:  # Rated region\n",
    "                wind_generation[i] = 150 + np.random.normal(0, 10)\n",
    "            else:  # Cut-out speed\n",
    "                wind_generation[i] = 0\n",
    "        \n",
    "        wind_generation = np.maximum(0, wind_generation)\n",
    "        \n",
    "        # Complex energy demand modeling with more patterns\n",
    "        demand_base = 550\n",
    "        daily_residential = 120 * np.maximum(0, np.sin((time_hours % 24 - 7) * np.pi / 11))\n",
    "        daily_commercial = 180 * np.maximum(0, np.sin((time_hours % 24 - 5) * np.pi / 14))\n",
    "        weekly_pattern = 80 * np.sin((time_hours % (24*7)) * 2 * np.pi / (24*7))\n",
    "        \n",
    "        heating_demand = np.maximum(0, (18 - base_temperature) * 15)\n",
    "        cooling_demand = np.maximum(0, (base_temperature - 22) * 20)\n",
    "        \n",
    "        business_hours = ((dates.hour >= 8) & (dates.hour <= 18) & \n",
    "                         (dates.dayofweek < 5)).astype(int)\n",
    "        industrial_demand = business_hours * 100 + np.random.normal(0, 20, hours)\n",
    "        \n",
    "        renewable_total = solar_generation + wind_generation\n",
    "        grid_tied_reduction = renewable_total * 0.12\n",
    "        \n",
    "        # Add demand persistence for better LSTM learning\n",
    "        demand_noise = np.random.normal(0, 30, hours)\n",
    "        for i in range(1, hours):\n",
    "            demand_noise[i] += 0.4 * demand_noise[i-1]  # Autocorrelation\n",
    "        \n",
    "        total_demand = (demand_base + daily_residential + daily_commercial + \n",
    "                       weekly_pattern + heating_demand + cooling_demand + \n",
    "                       industrial_demand - grid_tied_reduction + demand_noise)\n",
    "        total_demand = np.maximum(350, total_demand)\n",
    "        \n",
    "        # Natural gas generation\n",
    "        supply_shortfall = np.maximum(0, total_demand - renewable_total - 250)\n",
    "        natural_gas_generation = supply_shortfall * 0.8 + np.random.normal(0, 25, hours)\n",
    "        natural_gas_generation = np.maximum(0, natural_gas_generation)\n",
    "        \n",
    "        # Grid frequency\n",
    "        total_supply = renewable_total + natural_gas_generation + 250\n",
    "        frequency_deviation = (total_supply - total_demand) * 0.0008\n",
    "        grid_frequency = 50.0 + frequency_deviation + np.random.normal(0, 0.015, hours)\n",
    "        grid_frequency = np.clip(grid_frequency, 49.7, 50.3)\n",
    "        \n",
    "        # Energy price with more complexity\n",
    "        demand_factor = (total_demand - total_demand.mean()) / total_demand.std() * 12\n",
    "        renewable_factor = -(renewable_total - renewable_total.mean()) / renewable_total.std() * 8\n",
    "        gas_price_factor = (natural_gas_generation - natural_gas_generation.mean()) / natural_gas_generation.std() * 6\n",
    "        volatility = np.random.normal(0, 4, hours)\n",
    "        \n",
    "        energy_price = 45 + demand_factor + renewable_factor + gas_price_factor + volatility\n",
    "        energy_price = np.maximum(15, energy_price)\n",
    "        \n",
    "        # Create comprehensive dataset with ENHANCED FEATURES\n",
    "        energy_data = pd.DataFrame({\n",
    "            # Primary targets\n",
    "            'energy_demand': total_demand,\n",
    "            'solar_generation': solar_generation,\n",
    "            'wind_generation': wind_generation,\n",
    "            'natural_gas_generation': natural_gas_generation,\n",
    "            \n",
    "            # Derived variables\n",
    "            'total_renewable': renewable_total,\n",
    "            'total_generation': renewable_total + natural_gas_generation + 250,\n",
    "            'renewable_penetration': renewable_total / total_demand * 100,\n",
    "            'supply_demand_balance': (renewable_total + natural_gas_generation + 250) - total_demand,\n",
    "            \n",
    "            # Weather and external factors\n",
    "            'temperature': base_temperature,\n",
    "            'wind_speed': wind_speed,\n",
    "            'cloud_cover': cloud_cover,\n",
    "            \n",
    "            # System indicators\n",
    "            'grid_frequency': grid_frequency,\n",
    "            'energy_price': energy_price,\n",
    "            \n",
    "            # Temporal features\n",
    "            'hour': dates.hour,\n",
    "            'day_of_week': dates.dayofweek,\n",
    "            'month': dates.month,\n",
    "            'day_of_year': dates.dayofyear,\n",
    "            'is_weekend': dates.dayofweek >= 5,\n",
    "            'is_business_hour': business_hours,\n",
    "            'is_peak_hour': dates.hour.isin([17, 18, 19, 20]),\n",
    "            \n",
    "            # Enhanced cyclical encodings\n",
    "            'hour_sin': np.sin(2 * np.pi * dates.hour / 24),\n",
    "            'hour_cos': np.cos(2 * np.pi * dates.hour / 24),\n",
    "            'day_sin': np.sin(2 * np.pi * dates.dayofweek / 7),\n",
    "            'day_cos': np.cos(2 * np.pi * dates.dayofweek / 7),\n",
    "            'month_sin': np.sin(2 * np.pi * dates.month / 12),\n",
    "            'month_cos': np.cos(2 * np.pi * dates.month / 12),\n",
    "            'year_sin': np.sin(2 * np.pi * dates.dayofyear / 365),\n",
    "            'year_cos': np.cos(2 * np.pi * dates.dayofyear / 365)\n",
    "        }, index=dates)\n",
    "        \n",
    "        data_source = \"Generated enhanced multi-variate sample data\"\n",
    "    \n",
    "    # ADVANCED FEATURE ENGINEERING\n",
    "    energy_data = add_advanced_features(energy_data)\n",
    "    \n",
    "    # Validation summary\n",
    "    preparation_summary = {\n",
    "        'total_records': len(energy_data),\n",
    "        'date_range': {\n",
    "            'start': energy_data.index.min(),\n",
    "            'end': energy_data.index.max()\n",
    "        },\n",
    "        'variables': {\n",
    "            'total_variables': len(energy_data.columns),\n",
    "            'target_variables': ['energy_demand', 'solar_generation', 'wind_generation'],\n",
    "        },\n",
    "        'data_quality': {\n",
    "            'missing_values': energy_data.isnull().sum().sum(),\n",
    "            'data_completeness': (1 - energy_data.isnull().sum().sum() / \n",
    "                                (len(energy_data) * len(energy_data.columns))) * 100\n",
    "        },\n",
    "        'data_source': data_source\n",
    "    }\n",
    "    \n",
    "    print(f\"Data source: {data_source}\")\n",
    "    print(f\"Dataset shape: {energy_data.shape}\")\n",
    "    print(f\"Date range: {energy_data.index.min()} to {energy_data.index.max()}\")\n",
    "    print(f\"Data completeness: {preparation_summary['data_quality']['data_completeness']:.1f}%\")\n",
    "    \n",
    "    return energy_data, preparation_summary\n",
    "\n",
    "def add_advanced_features(data):\n",
    "    \"\"\"Add advanced features for better LSTM performance.\"\"\"\n",
    "    print(\"Adding advanced features for enhanced LSTM performance...\")\n",
    "    \n",
    "    # Lag features for temporal dependencies\n",
    "    for col in ['energy_demand', 'solar_generation', 'wind_generation']:\n",
    "        if col in data.columns:\n",
    "            data[f'{col}_lag_1h'] = data[col].shift(1)\n",
    "            data[f'{col}_lag_6h'] = data[col].shift(6)\n",
    "            data[f'{col}_lag_24h'] = data[col].shift(24)\n",
    "            data[f'{col}_lag_48h'] = data[col].shift(48)\n",
    "            data[f'{col}_lag_168h'] = data[col].shift(168)  # Weekly lag\n",
    "    \n",
    "    # Rolling statistics\n",
    "    for col in ['energy_demand', 'solar_generation', 'wind_generation']:\n",
    "        if col in data.columns:\n",
    "            data[f'{col}_rolling_mean_6h'] = data[col].rolling(window=6).mean()\n",
    "            data[f'{col}_rolling_mean_24h'] = data[col].rolling(window=24).mean()\n",
    "            data[f'{col}_rolling_std_24h'] = data[col].rolling(window=24).std()\n",
    "            data[f'{col}_rolling_max_24h'] = data[col].rolling(window=24).max()\n",
    "            data[f'{col}_rolling_min_24h'] = data[col].rolling(window=24).min()\n",
    "    \n",
    "    # Interaction features\n",
    "    if 'temperature' in data.columns and 'hour' in data.columns:\n",
    "        data['temp_hour_interaction'] = data['temperature'] * data['hour']\n",
    "    \n",
    "    if 'renewable_penetration' in data.columns and 'energy_demand' in data.columns:\n",
    "        data['renewable_demand_ratio'] = data['renewable_penetration'] / (data['energy_demand'] + 1e-8)\n",
    "    \n",
    "    # Weather-based features\n",
    "    if 'temperature' in data.columns:\n",
    "        data['heating_degree_hours'] = np.maximum(0, 18 - data['temperature'])\n",
    "        data['cooling_degree_hours'] = np.maximum(0, data['temperature'] - 22)\n",
    "    \n",
    "    # Time-based features\n",
    "    data['quarter'] = data.index.quarter\n",
    "    data['week_of_year'] = data.index.isocalendar().week\n",
    "    \n",
    "    # Advanced cyclical features\n",
    "    data['quarter_sin'] = np.sin(2 * np.pi * data['quarter'] / 4)\n",
    "    data['quarter_cos'] = np.cos(2 * np.pi * data['quarter'] / 4)\n",
    "    \n",
    "    # Remove rows with NaN values from lag features\n",
    "    data = data.dropna()\n",
    "    \n",
    "    print(f\"Enhanced dataset shape after feature engineering: {data.shape}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "def create_multi_variate_sequences(data, target_cols, feature_cols, \n",
    "                                  sequence_length=168, forecast_horizons=[1, 6, 24]):  # Extended to weekly\n",
    "    \"\"\"Create multi-variate sequences for advanced LSTM with extended sequence length.\"\"\"\n",
    "    print(f\"Creating multi-variate sequences for advanced LSTM modeling\")\n",
    "    print(f\"Target variables: {target_cols}\")\n",
    "    print(f\"Feature variables: {len(feature_cols)} features\")\n",
    "    print(f\"Sequence length: {sequence_length} hours (EXTENDED FOR WEEKLY PATTERNS)\")\n",
    "    print(f\"Forecast horizons: {forecast_horizons}\")\n",
    "    \n",
    "    # Prepare data arrays\n",
    "    feature_data = data[feature_cols].values\n",
    "    target_data = data[target_cols].values\n",
    "    timestamps = data.index\n",
    "    \n",
    "    # Create sequences\n",
    "    X_sequences = []\n",
    "    y_multi_target_horizon = []\n",
    "    sequence_timestamps = []\n",
    "    \n",
    "    max_horizon = max(forecast_horizons)\n",
    "    \n",
    "    for i in range(sequence_length, len(data) - max_horizon):\n",
    "        # Extract feature sequence\n",
    "        feature_sequence = feature_data[i-sequence_length:i]\n",
    "        X_sequences.append(feature_sequence)\n",
    "        \n",
    "        # Extract multi-target, multi-horizon outputs\n",
    "        multi_target_horizons = []\n",
    "        \n",
    "        for target_idx in range(len(target_cols)):\n",
    "            target_horizons = []\n",
    "            for horizon in forecast_horizons:\n",
    "                target_value = target_data[i + horizon - 1, target_idx]\n",
    "                target_horizons.append(target_value)\n",
    "            multi_target_horizons.extend(target_horizons)\n",
    "        \n",
    "        y_multi_target_horizon.append(multi_target_horizons)\n",
    "        sequence_timestamps.append(timestamps[i])\n",
    "    \n",
    "    X_sequences = np.array(X_sequences)\n",
    "    y_multi_target_horizon = np.array(y_multi_target_horizon)\n",
    "    sequence_timestamps = np.array(sequence_timestamps)\n",
    "    \n",
    "    print(f\"Created multi-variate sequences:\")\n",
    "    print(f\"  X_shape: {X_sequences.shape}\")\n",
    "    print(f\"  y_shape: {y_multi_target_horizon.shape}\")\n",
    "    print(f\"  Output structure: {len(target_cols)} targets × {len(forecast_horizons)} horizons\")\n",
    "    \n",
    "    return X_sequences, y_multi_target_horizon, sequence_timestamps\n",
    "\n",
    "# =============================================================================\n",
    "# ENHANCED LSTM ARCHITECTURES FOR 85%+ ACCURACY\n",
    "# =============================================================================\n",
    "\n",
    "def build_deep_attention_lstm_model(input_shape, output_size, \n",
    "                                   lstm_units=[128, 64, 32], attention_units=64,\n",
    "                                   dropout_rate=0.1, learning_rate=0.0005):\n",
    "    \"\"\"Build DEEPER attention-based LSTM model for higher accuracy.\"\"\"\n",
    "    print(f\"Building DEEP attention-based LSTM model for 85%+ accuracy\")\n",
    "    print(f\"Input shape: {input_shape}\")\n",
    "    print(f\"Output size: {output_size}\")\n",
    "    print(f\"Architecture: DEEPER with {len(lstm_units)} LSTM layers\")\n",
    "    \n",
    "    if not TENSORFLOW_AVAILABLE:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Define input layer\n",
    "        inputs = Input(shape=input_shape, name='energy_sequence_input')\n",
    "        \n",
    "        # Multi-layer LSTM with return sequences\n",
    "        x = inputs\n",
    "        \n",
    "        for i, units in enumerate(lstm_units):\n",
    "            x = LSTM(\n",
    "                units=units,\n",
    "                return_sequences=True,  # Always true for attention\n",
    "                dropout=dropout_rate,\n",
    "                recurrent_dropout=dropout_rate,\n",
    "                name=f'lstm_attention_layer_{i+1}'\n",
    "            )(x)\n",
    "            \n",
    "            x = BatchNormalization(name=f'batch_norm_lstm_{i+1}')(x)\n",
    "            print(f\"  DEEP LSTM Layer {i+1}: {units} units\")\n",
    "        \n",
    "        # Enhanced attention mechanism\n",
    "        attention_weights = Dense(attention_units, activation='tanh', name='attention_tanh')(x)\n",
    "        attention_weights = Dense(1, activation='softmax', name='attention_weights')(attention_weights)\n",
    "        \n",
    "        # Apply attention\n",
    "        attended_features = layers.Multiply(name='attention_multiply')([x, attention_weights])\n",
    "        \n",
    "        # Global pooling\n",
    "        global_features = layers.GlobalAveragePooling1D(name='global_attention_pool')(attended_features)\n",
    "        \n",
    "        # Deeper dense layers\n",
    "        dense_1 = Dense(128, activation='relu', name='dense_attention_1')(global_features)\n",
    "        dense_1 = Dropout(dropout_rate, name='dropout_dense_1')(dense_1)\n",
    "        dense_1 = BatchNormalization(name='batch_norm_dense_1')(dense_1)\n",
    "        \n",
    "        dense_2 = Dense(64, activation='relu', name='dense_attention_2')(dense_1)\n",
    "        dense_2 = Dropout(dropout_rate, name='dropout_dense_2')(dense_2)\n",
    "        dense_2 = BatchNormalization(name='batch_norm_dense_2')(dense_2)\n",
    "        \n",
    "        dense_3 = Dense(32, activation='relu', name='dense_attention_3')(dense_2)\n",
    "        dense_3 = Dropout(dropout_rate, name='dropout_dense_3')(dense_3)\n",
    "        \n",
    "        # Output layer\n",
    "        outputs = Dense(output_size, activation='linear', name='attention_forecast_output')(dense_3)\n",
    "        \n",
    "        # Create model\n",
    "        model = Model(inputs=inputs, outputs=outputs, name='DeepAttentionLSTM_EnergyForecaster')\n",
    "        \n",
    "        # Use lower learning rate for stability\n",
    "        optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='mse',\n",
    "            metrics=['mae', 'mape']\n",
    "        )\n",
    "        \n",
    "        print(f\"  DEEP Attention-based LSTM model compiled successfully\")\n",
    "        print(f\"  Total parameters: {model.count_params():,}\")\n",
    "        return model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error building deep attention LSTM: {e}\")\n",
    "        return None\n",
    "\n",
    "def build_hybrid_cnn_lstm_model(input_shape, output_size,\n",
    "                               conv_filters=[64, 32], lstm_units=[128, 64],\n",
    "                               dropout_rate=0.1, learning_rate=0.0005):\n",
    "    \"\"\"Build hybrid CNN-LSTM model for feature extraction and temporal modeling.\"\"\"\n",
    "    print(f\"Building HYBRID CNN-LSTM model for enhanced feature extraction\")\n",
    "    \n",
    "    if not TENSORFLOW_AVAILABLE:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        inputs = Input(shape=input_shape, name='hybrid_input')\n",
    "        \n",
    "        # CNN for feature extraction\n",
    "        x = inputs\n",
    "        for i, filters in enumerate(conv_filters):\n",
    "            x = Conv1D(filters=filters, kernel_size=3, activation='relu', \n",
    "                      name=f'conv1d_{i+1}')(x)\n",
    "            x = BatchNormalization(name=f'conv_bn_{i+1}')(x)\n",
    "            x = Dropout(dropout_rate, name=f'conv_dropout_{i+1}')(x)\n",
    "            print(f\"  CNN Layer {i+1}: {filters} filters\")\n",
    "        \n",
    "        # LSTM for temporal modeling\n",
    "        for i, units in enumerate(lstm_units):\n",
    "            return_sequences = (i < len(lstm_units) - 1)\n",
    "            x = LSTM(units=units, return_sequences=return_sequences,\n",
    "                    dropout=dropout_rate, recurrent_dropout=dropout_rate,\n",
    "                    name=f'lstm_layer_{i+1}')(x)\n",
    "            if return_sequences:\n",
    "                x = BatchNormalization(name=f'lstm_bn_{i+1}')(x)\n",
    "            print(f\"  LSTM Layer {i+1}: {units} units\")\n",
    "        \n",
    "        # Dense layers\n",
    "        x = Dense(128, activation='relu', name='dense_1')(x)\n",
    "        x = Dropout(dropout_rate, name='dense_dropout_1')(x)\n",
    "        x = BatchNormalization(name='dense_bn_1')(x)\n",
    "        \n",
    "        x = Dense(64, activation='relu', name='dense_2')(x)\n",
    "        x = Dropout(dropout_rate, name='dense_dropout_2')(x)\n",
    "        \n",
    "        outputs = Dense(output_size, activation='linear', name='hybrid_output')(x)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=outputs, name='HybridCNN_LSTM')\n",
    "        \n",
    "        optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss='mse', metrics=['mae', 'mape'])\n",
    "        \n",
    "        print(f\"  Hybrid CNN-LSTM model compiled successfully\")\n",
    "        print(f\"  Total parameters: {model.count_params():,}\")\n",
    "        return model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error building hybrid CNN-LSTM: {e}\")\n",
    "        return None\n",
    "\n",
    "def build_transformer_lstm_model(input_shape, output_size,\n",
    "                                lstm_units=[128, 64], num_heads=8, ff_dim=128,\n",
    "                                dropout_rate=0.1, learning_rate=0.0005):\n",
    "    \"\"\"Build Transformer-enhanced LSTM model.\"\"\"\n",
    "    print(f\"Building TRANSFORMER-enhanced LSTM model\")\n",
    "    \n",
    "    if not TENSORFLOW_AVAILABLE:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        inputs = Input(shape=input_shape, name='transformer_input')\n",
    "        \n",
    "        # LSTM processing\n",
    "        x = inputs\n",
    "        for i, units in enumerate(lstm_units):\n",
    "            x = LSTM(units=units, return_sequences=True,\n",
    "                    dropout=dropout_rate, recurrent_dropout=dropout_rate,\n",
    "                    name=f'lstm_layer_{i+1}')(x)\n",
    "            x = BatchNormalization(name=f'lstm_bn_{i+1}')(x)\n",
    "            print(f\"  LSTM Layer {i+1}: {units} units\")\n",
    "        \n",
    "        # Multi-head attention\n",
    "        attention_output = MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=64, name='multi_head_attention'\n",
    "        )(x, x)\n",
    "        \n",
    "        # Add & Norm\n",
    "        x = layers.Add(name='add_attention')([x, attention_output])\n",
    "        x = LayerNormalization(name='norm_attention')(x)\n",
    "        \n",
    "        # Feed Forward\n",
    "        ff_output = Dense(ff_dim, activation='relu', name='ff_1')(x)\n",
    "        ff_output = Dropout(dropout_rate, name='ff_dropout')(ff_output)\n",
    "        ff_output = Dense(lstm_units[-1], name='ff_2')(ff_output)\n",
    "        \n",
    "        # Add & Norm\n",
    "        x = layers.Add(name='add_ff')([x, ff_output])\n",
    "        x = LayerNormalization(name='norm_ff')(x)\n",
    "        \n",
    "        # Global pooling\n",
    "        x = layers.GlobalAveragePooling1D(name='global_pool')(x)\n",
    "        \n",
    "        # Dense layers\n",
    "        x = Dense(128, activation='relu', name='dense_1')(x)\n",
    "        x = Dropout(dropout_rate, name='dense_dropout_1')(x)\n",
    "        x = BatchNormalization(name='dense_bn_1')(x)\n",
    "        \n",
    "        x = Dense(64, activation='relu', name='dense_2')(x)\n",
    "        x = Dropout(dropout_rate, name='dense_dropout_2')(x)\n",
    "        \n",
    "        outputs = Dense(output_size, activation='linear', name='transformer_output')(x)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=outputs, name='TransformerLSTM')\n",
    "        \n",
    "        optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss='mse', metrics=['mae', 'mape'])\n",
    "        \n",
    "        print(f\"  Transformer-LSTM model compiled successfully\")\n",
    "        print(f\"  Total parameters: {model.count_params():,}\")\n",
    "        return model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error building Transformer-LSTM: {e}\")\n",
    "        return None\n",
    "\n",
    "# =============================================================================\n",
    "# ADVANCED TRAINING STRATEGIES FOR 85%+ ACCURACY\n",
    "# =============================================================================\n",
    "\n",
    "def create_advanced_callbacks_for_accuracy(model_name, patience=50):\n",
    "    \"\"\"Create advanced callbacks optimized for high accuracy.\"\"\"\n",
    "    print(f\"Setting up ADVANCED callbacks for {model_name} (targeting 85%+ accuracy)...\")\n",
    "    \n",
    "    model_dir = '../../models/advanced_lstm'\n",
    "    log_dir = '../../results/logs/advanced_training'\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    callbacks_list = []\n",
    "    \n",
    "    # Extended early stopping for better convergence\n",
    "    early_stopping = callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=patience,  # Increased patience\n",
    "        restore_best_weights=True,\n",
    "        verbose=1,\n",
    "        mode='min',\n",
    "        min_delta=1e-7  # Smaller min_delta for fine-tuning\n",
    "    )\n",
    "    callbacks_list.append(early_stopping)\n",
    "    \n",
    "    # Model checkpoint\n",
    "    checkpoint_path = os.path.join(model_dir, f'{model_name}_best.h5')\n",
    "    model_checkpoint = callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,\n",
    "        verbose=1,\n",
    "        mode='min'\n",
    "    )\n",
    "    callbacks_list.append(model_checkpoint)\n",
    "    \n",
    "    # Advanced learning rate scheduling\n",
    "    lr_scheduler = callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.3,  # More aggressive reduction\n",
    "        patience=15,  # More patience before reduction\n",
    "        min_lr=1e-8,  # Lower minimum learning rate\n",
    "        verbose=1,\n",
    "        mode='min'\n",
    "    )\n",
    "    callbacks_list.append(lr_scheduler)\n",
    "    \n",
    "    # Cyclical learning rate for better convergence\n",
    "    try:\n",
    "        from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "        \n",
    "        def cyclical_lr_schedule(epoch, lr):\n",
    "            \"\"\"Cyclical learning rate schedule.\"\"\"\n",
    "            cycle_length = 20\n",
    "            cycle = epoch // cycle_length\n",
    "            x = epoch % cycle_length\n",
    "            if x < cycle_length // 2:\n",
    "                return lr * (1 + x / (cycle_length // 2))\n",
    "            else:\n",
    "                return lr * (2 - x / (cycle_length // 2))\n",
    "        \n",
    "        cyclical_lr = LearningRateScheduler(cyclical_lr_schedule, verbose=0)\n",
    "        callbacks_list.append(cyclical_lr)\n",
    "        \n",
    "    except:\n",
    "        print(\"  Cyclical LR not available, using standard scheduling\")\n",
    "    \n",
    "    # CSV logger\n",
    "    csv_logger = callbacks.CSVLogger(\n",
    "        os.path.join(log_dir, f'{model_name}_training_metrics.csv'),\n",
    "        append=True\n",
    "    )\n",
    "    callbacks_list.append(csv_logger)\n",
    "    \n",
    "    print(f\"  ADVANCED callbacks configured: {len(callbacks_list)} callbacks\")\n",
    "    print(f\"  Early stopping patience: {patience} epochs\")\n",
    "    print(f\"  Advanced learning rate scheduling enabled\")\n",
    "    \n",
    "    return callbacks_list\n",
    "\n",
    "def train_advanced_model_for_accuracy(model, model_name, X_train, y_train, X_val, y_val,\n",
    "                                    epochs=150, batch_size=16):  # Extended training\n",
    "    \"\"\"Train advanced LSTM model with optimization for 85%+ accuracy.\"\"\"\n",
    "    print(f\"Training ADVANCED {model_name} model for 85%+ accuracy...\")\n",
    "    print(f\"  Training samples: {X_train.shape[0]}\")\n",
    "    print(f\"  Validation samples: {X_val.shape[0]}\")\n",
    "    print(f\"  EXTENDED TRAINING: {epochs} epochs (vs previous 20)\")\n",
    "    print(f\"  SMALLER BATCH SIZE: {batch_size} (for better gradients)\")\n",
    "    \n",
    "    if not hasattr(model, 'fit'):\n",
    "        print(f\"  Mock training completed for {model_name}\")\n",
    "        return None\n",
    "    \n",
    "    # Create advanced callbacks\n",
    "    advanced_callbacks = create_advanced_callbacks_for_accuracy(model_name, patience=50)\n",
    "    \n",
    "    training_start = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        # Train model with extended epochs\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=epochs,  # Extended training\n",
    "            batch_size=batch_size,  # Smaller batch size\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=advanced_callbacks,\n",
    "            verbose=1,\n",
    "            shuffle=False  # Maintain temporal order\n",
    "        )\n",
    "        \n",
    "        training_end = datetime.now()\n",
    "        training_duration = training_end - training_start\n",
    "        \n",
    "        print(f\"  EXTENDED TRAINING completed: {training_duration}\")\n",
    "        print(f\"  Final training loss: {history.history['loss'][-1]:.6f}\")\n",
    "        print(f\"  Final validation loss: {history.history['val_loss'][-1]:.6f}\")\n",
    "        print(f\"  Best epoch: {np.argmin(history.history['val_loss']) + 1}\")\n",
    "        print(f\"  Training epochs completed: {len(history.history['loss'])}\")\n",
    "        \n",
    "        return history\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Training error: {e}\")\n",
    "        return None\n",
    "\n",
    "def calculate_r2_from_val_loss(val_loss, data_variance):\n",
    "    \"\"\"Estimate R² from validation loss.\"\"\"\n",
    "    # R² = 1 - (SSres / SStot) ≈ 1 - (val_loss / var(y))\n",
    "    r2_estimate = max(0, 1 - (val_loss / data_variance))\n",
    "    return r2_estimate\n",
    "\n",
    "# =============================================================================\n",
    "# ENHANCED ENSEMBLE METHODS FOR 85%+ ACCURACY\n",
    "# =============================================================================\n",
    "\n",
    "def build_super_ensemble_models(input_shape, output_size, num_models=5):\n",
    "    \"\"\"Build super ensemble with diverse architectures for maximum accuracy.\"\"\"\n",
    "    print(f\"Building SUPER ENSEMBLE with {num_models} diverse models for 85%+ accuracy\")\n",
    "    \n",
    "    ensemble_models = []\n",
    "    model_configs = [\n",
    "        {\n",
    "            'name': 'deep_attention_lstm',\n",
    "            'builder': build_deep_attention_lstm_model,\n",
    "            'params': {'lstm_units': [128, 64, 32], 'attention_units': 64}\n",
    "        },\n",
    "        {\n",
    "            'name': 'hybrid_cnn_lstm',\n",
    "            'builder': build_hybrid_cnn_lstm_model,\n",
    "            'params': {'conv_filters': [64, 32], 'lstm_units': [128, 64]}\n",
    "        },\n",
    "        {\n",
    "            'name': 'transformer_lstm',\n",
    "            'builder': build_transformer_lstm_model,\n",
    "            'params': {'lstm_units': [128, 64], 'num_heads': 8}\n",
    "        },\n",
    "        {\n",
    "            'name': 'bidirectional_lstm',\n",
    "            'builder': build_bidirectional_lstm_model,\n",
    "            'params': {'lstm_units': [128, 64]}\n",
    "        },\n",
    "        {\n",
    "            'name': 'deep_multivariate_lstm',\n",
    "            'builder': build_deep_multivariate_lstm_model,\n",
    "            'params': {'lstm_units': [128, 64, 32]}\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for i, config in enumerate(model_configs[:num_models]):\n",
    "        print(f\"  Building ensemble member {i+1}: {config['name']}\")\n",
    "        \n",
    "        try:\n",
    "            model = config['builder'](\n",
    "                input_shape=input_shape,\n",
    "                output_size=output_size,\n",
    "                **config['params']\n",
    "            )\n",
    "            \n",
    "            if model is not None:\n",
    "                ensemble_models.append({\n",
    "                    'model': model,\n",
    "                    'name': config['name'],\n",
    "                    'config': config\n",
    "                })\n",
    "                print(f\"    ✓ {config['name']} built successfully\")\n",
    "            else:\n",
    "                print(f\"    ✗ {config['name']} failed to build\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"    ✗ Error building {config['name']}: {e}\")\n",
    "    \n",
    "    print(f\"  SUPER ENSEMBLE built with {len(ensemble_models)} diverse models\")\n",
    "    return ensemble_models\n",
    "\n",
    "def build_bidirectional_lstm_model(input_shape, output_size, lstm_units=[128, 64],\n",
    "                                  dropout_rate=0.1, learning_rate=0.0005):\n",
    "    \"\"\"Build bidirectional LSTM model.\"\"\"\n",
    "    print(f\"Building BIDIRECTIONAL LSTM model\")\n",
    "    \n",
    "    if not TENSORFLOW_AVAILABLE:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        inputs = Input(shape=input_shape, name='bidirectional_input')\n",
    "        \n",
    "        x = inputs\n",
    "        for i, units in enumerate(lstm_units):\n",
    "            return_sequences = (i < len(lstm_units) - 1)\n",
    "            \n",
    "            x = Bidirectional(LSTM(\n",
    "                units=units,\n",
    "                return_sequences=return_sequences,\n",
    "                dropout=dropout_rate,\n",
    "                recurrent_dropout=dropout_rate,\n",
    "                name=f'lstm_layer_{i+1}'\n",
    "            ), name=f'bidirectional_{i+1}')(x)\n",
    "            \n",
    "            if return_sequences:\n",
    "                x = BatchNormalization(name=f'bn_{i+1}')(x)\n",
    "            print(f\"  Bidirectional LSTM Layer {i+1}: {units*2} units (forward + backward)\")\n",
    "        \n",
    "        # Dense layers\n",
    "        x = Dense(128, activation='relu', name='dense_1')(x)\n",
    "        x = Dropout(dropout_rate, name='dropout_1')(x)\n",
    "        x = BatchNormalization(name='bn_dense_1')(x)\n",
    "        \n",
    "        x = Dense(64, activation='relu', name='dense_2')(x)\n",
    "        x = Dropout(dropout_rate, name='dropout_2')(x)\n",
    "        \n",
    "        outputs = Dense(output_size, activation='linear', name='bidirectional_output')(x)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=outputs, name='BidirectionalLSTM')\n",
    "        \n",
    "        optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss='mse', metrics=['mae', 'mape'])\n",
    "        \n",
    "        print(f\"  Bidirectional LSTM compiled successfully\")\n",
    "        return model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error building Bidirectional LSTM: {e}\")\n",
    "        return None\n",
    "\n",
    "def build_deep_multivariate_lstm_model(input_shape, output_size, lstm_units=[128, 64, 32],\n",
    "                                      dropout_rate=0.1, learning_rate=0.0005):\n",
    "    \"\"\"Build deep multivariate LSTM model.\"\"\"\n",
    "    print(f\"Building DEEP MULTIVARIATE LSTM model\")\n",
    "    \n",
    "    if not TENSORFLOW_AVAILABLE:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        inputs = Input(shape=input_shape, name='deep_multivariate_input')\n",
    "        \n",
    "        # Multiple parallel LSTM branches\n",
    "        branch_outputs = []\n",
    "        \n",
    "        for branch_idx in range(3):  # 3 branches for diversity\n",
    "            x = inputs\n",
    "            \n",
    "            for i, units in enumerate(lstm_units):\n",
    "                return_sequences = (i < len(lstm_units) - 1)\n",
    "                \n",
    "                x = LSTM(\n",
    "                    units=units + branch_idx * 16,  # Slight variation per branch\n",
    "                    return_sequences=return_sequences,\n",
    "                    dropout=dropout_rate + branch_idx * 0.02,\n",
    "                    recurrent_dropout=dropout_rate,\n",
    "                    name=f'branch_{branch_idx}_lstm_{i+1}'\n",
    "                )(x)\n",
    "                \n",
    "                if return_sequences:\n",
    "                    x = BatchNormalization(name=f'branch_{branch_idx}_bn_{i+1}')(x)\n",
    "            \n",
    "            branch_outputs.append(x)\n",
    "            print(f\"  Branch {branch_idx + 1}: {[u + branch_idx * 16 for u in lstm_units]} units\")\n",
    "        \n",
    "        # Combine branches\n",
    "        combined = Concatenate(name='combine_branches')(branch_outputs)\n",
    "        \n",
    "        # Dense layers\n",
    "        x = Dense(256, activation='relu', name='dense_1')(combined)\n",
    "        x = Dropout(dropout_rate, name='dropout_1')(x)\n",
    "        x = BatchNormalization(name='bn_1')(x)\n",
    "        \n",
    "        x = Dense(128, activation='relu', name='dense_2')(x)\n",
    "        x = Dropout(dropout_rate, name='dropout_2')(x)\n",
    "        x = BatchNormalization(name='bn_2')(x)\n",
    "        \n",
    "        x = Dense(64, activation='relu', name='dense_3')(x)\n",
    "        x = Dropout(dropout_rate, name='dropout_3')(x)\n",
    "        \n",
    "        outputs = Dense(output_size, activation='linear', name='deep_multivariate_output')(x)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=outputs, name='DeepMultivariateLSTM')\n",
    "        \n",
    "        optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss='mse', metrics=['mae', 'mape'])\n",
    "        \n",
    "        print(f\"  Deep Multivariate LSTM compiled successfully\")\n",
    "        print(f\"  Total parameters: {model.count_params():,}\")\n",
    "        return model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error building Deep Multivariate LSTM: {e}\")\n",
    "        return None\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EXECUTION PIPELINE FOR 85%+ ACCURACY\n",
    "# =============================================================================\n",
    "\n",
    "def main_optimized_for_accuracy():\n",
    "    \"\"\"Main execution pipeline optimized for achieving 85%+ accuracy.\"\"\"\n",
    "    print(\"\\nEXECUTING OPTIMIZED ADVANCED LSTM PIPELINE FOR 85%+ ACCURACY\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Load data with enhanced features\n",
    "    print(\"\\nSTEP 1: ENHANCED DATA LOADING AND PREPARATION\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    energy_data, data_prep_summary = load_comprehensive_energy_data()\n",
    "    \n",
    "    # Check data size - don't sample too aggressively as we need data for accuracy\n",
    "    print(f\"Dataset size: {len(energy_data)} records\")\n",
    "    \n",
    "    if len(energy_data) > 50000:\n",
    "        print(\"Large dataset detected - moderate sampling to preserve patterns...\")\n",
    "        sample_rate = max(1, len(energy_data) // 30000)  # Less aggressive sampling\n",
    "        energy_data = energy_data.iloc[::sample_rate].copy()\n",
    "        print(f\"Sampled dataset size: {len(energy_data)} records (sample rate: 1/{sample_rate})\")\n",
    "    \n",
    "    # Enhanced parameters for better accuracy\n",
    "    SEQUENCE_LENGTH = 168  # Weekly patterns (7 days * 24 hours)\n",
    "    FORECAST_HORIZONS = [1, 6, 24]  # Multiple horizons\n",
    "    \n",
    "    print(f\"ENHANCED PARAMETERS for 85%+ accuracy:\")\n",
    "    print(f\"  Sequence length: {SEQUENCE_LENGTH} hours (WEEKLY PATTERNS)\")\n",
    "    print(f\"  Forecast horizons: {FORECAST_HORIZONS}\")\n",
    "    \n",
    "    # Enhanced target and feature selection\n",
    "    print(f\"Available columns: {len(energy_data.columns)} total\")\n",
    "    \n",
    "    # Find targets\n",
    "    possible_target_names = {\n",
    "        'energy_demand': ['energy_demand', 'demand', 'load', 'consumption'],\n",
    "        'solar_generation': ['solar_generation', 'solar', 'pv', 'photovoltaic'],\n",
    "        'wind_generation': ['wind_generation', 'wind']\n",
    "    }\n",
    "    \n",
    "    actual_targets = []\n",
    "    for target_key, possible_names in possible_target_names.items():\n",
    "        for name in possible_names:\n",
    "            if name in energy_data.columns:\n",
    "                actual_targets.append(name)\n",
    "                print(f\"Found target: {name}\")\n",
    "                break\n",
    "    \n",
    "    if not actual_targets:\n",
    "        numeric_cols = energy_data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        actual_targets = numeric_cols[:3]\n",
    "        print(f\"Using first 3 numeric columns as targets: {actual_targets}\")\n",
    "    \n",
    "    # Enhanced feature selection - use MORE features for better accuracy\n",
    "    available_features = []\n",
    "    for col in energy_data.columns:\n",
    "        if (energy_data[col].dtype in [np.float64, np.float32, np.int64, np.int32] and \n",
    "            col not in actual_targets):\n",
    "            available_features.append(col)\n",
    "    \n",
    "    # Use more features for higher accuracy (up to 25)\n",
    "    if len(available_features) > 25:\n",
    "        # Prioritize important features\n",
    "        important_features = []\n",
    "        for col in available_features:\n",
    "            col_lower = col.lower()\n",
    "            if any(keyword in col_lower for keyword in \n",
    "                   ['temp', 'wind', 'solar', 'hour', 'day', 'lag', 'rolling', 'renewable']):\n",
    "                important_features.append(col)\n",
    "        \n",
    "        # Fill remaining with other features\n",
    "        remaining_features = [col for col in available_features if col not in important_features]\n",
    "        available_features = important_features + remaining_features[:25-len(important_features)]\n",
    "        \n",
    "        print(f\"Selected {len(available_features)} ENHANCED features for higher accuracy\")\n",
    "    \n",
    "    print(f\"Selected targets: {actual_targets}\")\n",
    "    print(f\"Selected features: {len(available_features)} features\")\n",
    "    \n",
    "    # Create enhanced sequences\n",
    "    print(\"\\nSTEP 2: ENHANCED SEQUENCE CREATION\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    X_mv_sequences, y_mv_multi_horizon, mv_timestamps = create_multi_variate_sequences(\n",
    "        energy_data, actual_targets, available_features,\n",
    "        SEQUENCE_LENGTH, FORECAST_HORIZONS\n",
    "    )\n",
    "    \n",
    "    # Check memory and proceed more conservatively\n",
    "    memory_required_gb = (X_mv_sequences.nbytes + y_mv_multi_horizon.nbytes) / (1024**3)\n",
    "    print(f\"Memory required: {memory_required_gb:.2f} GB\")\n",
    "    \n",
    "    if memory_required_gb > 4.0:  # More generous memory allowance\n",
    "        print(\"High memory usage - selective sampling...\")\n",
    "        sample_size = min(len(X_mv_sequences), 15000)  # Keep more data\n",
    "        indices = np.random.choice(len(X_mv_sequences), sample_size, replace=False)\n",
    "        indices = np.sort(indices)\n",
    "        \n",
    "        X_mv_sequences = X_mv_sequences[indices]\n",
    "        y_mv_multi_horizon = y_mv_multi_horizon[indices]\n",
    "        mv_timestamps = mv_timestamps[indices]\n",
    "        \n",
    "        print(f\"Sampled to {len(X_mv_sequences)} sequences\")\n",
    "    \n",
    "    # Enhanced data splits for better training\n",
    "    print(\"\\nSTEP 3: ENHANCED DATA SPLITTING\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    total_sequences = len(X_mv_sequences)\n",
    "    train_size = int(0.75 * total_sequences)  # More training data\n",
    "    val_size = int(0.15 * total_sequences)\n",
    "    \n",
    "    X_train = X_mv_sequences[:train_size]\n",
    "    y_train = y_mv_multi_horizon[:train_size]\n",
    "    \n",
    "    X_val = X_mv_sequences[train_size:train_size + val_size]\n",
    "    y_val = y_mv_multi_horizon[train_size:train_size + val_size]\n",
    "    \n",
    "    X_test = X_mv_sequences[train_size + val_size:]\n",
    "    y_test = y_mv_multi_horizon[train_size + val_size:]\n",
    "    \n",
    "    print(f\"ENHANCED splits for 85%+ accuracy:\")\n",
    "    print(f\"  Training: {len(X_train)} sequences (75%)\")\n",
    "    print(f\"  Validation: {len(X_val)} sequences (15%)\")\n",
    "    print(f\"  Test: {len(X_test)} sequences (10%)\")\n",
    "    \n",
    "    # Enhanced normalization\n",
    "    print(\"\\nSTEP 4: ENHANCED DATA NORMALIZATION\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # More sophisticated normalization\n",
    "    feature_scaler = RobustScaler()  # Better for outliers\n",
    "    target_scaler = StandardScaler()\n",
    "    \n",
    "    original_shape = X_train.shape\n",
    "    X_train_reshaped = X_train.reshape(-1, X_train.shape[-1])\n",
    "    \n",
    "    feature_scaler.fit(X_train_reshaped)\n",
    "    \n",
    "    X_train_scaled = feature_scaler.transform(X_train_reshaped).reshape(original_shape)\n",
    "    X_val_scaled = feature_scaler.transform(X_val.reshape(-1, X_val.shape[-1])).reshape(X_val.shape)\n",
    "    X_test_scaled = feature_scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "    \n",
    "    y_train_scaled = target_scaler.fit_transform(y_train)\n",
    "    y_val_scaled = target_scaler.transform(y_val)\n",
    "    y_test_scaled = target_scaler.transform(y_test)\n",
    "    \n",
    "    print(f\"Enhanced normalization completed\")\n",
    "    print(f\"  Feature scaling: RobustScaler (better outlier handling)\")\n",
    "    print(f\"  Target scaling: StandardScaler\")\n",
    "    \n",
    "    # Calculate data variance for R² estimation\n",
    "    data_variance = np.var(y_val_scaled)\n",
    "    print(f\"  Data variance for R² estimation: {data_variance:.6f}\")\n",
    "    \n",
    "    # Build enhanced models\n",
    "    print(\"\\nSTEP 5: BUILDING ENHANCED MODEL ARCHITECTURES\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    input_shape = (SEQUENCE_LENGTH, len(available_features))\n",
    "    output_size = len(actual_targets) * len(FORECAST_HORIZONS)\n",
    "    \n",
    "    print(f\"Enhanced model configuration:\")\n",
    "    print(f\"  Input shape: {input_shape}\")\n",
    "    print(f\"  Output size: {output_size}\")\n",
    "    print(f\"  Architecture focus: DEEP networks for 85%+ accuracy\")\n",
    "    \n",
    "    # Build super ensemble\n",
    "    ensemble_models = build_super_ensemble_models(input_shape, output_size, num_models=3)\n",
    "    \n",
    "    print(\"Enhanced models built successfully\")\n",
    "    \n",
    "    # Enhanced training\n",
    "    print(\"\\nSTEP 6: ENHANCED MODEL TRAINING FOR 85%+ ACCURACY\")\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    training_results = {}\n",
    "    \n",
    "    # Extended training parameters\n",
    "    ENHANCED_EPOCHS = 150  # Much longer training\n",
    "    ENHANCED_BATCH_SIZE = 16  # Smaller for better gradients\n",
    "    \n",
    "    print(f\"ENHANCED TRAINING CONFIGURATION:\")\n",
    "    print(f\"  Epochs: {ENHANCED_EPOCHS} (vs previous 20)\")\n",
    "    print(f\"  Batch size: {ENHANCED_BATCH_SIZE} (smaller for better gradients)\")\n",
    "    print(f\"  Advanced callbacks: Extended patience, cyclical LR\")\n",
    "    \n",
    "    # Train each model in the ensemble\n",
    "    for i, model_info in enumerate(ensemble_models):\n",
    "        model_name = model_info['name']\n",
    "        model = model_info['model']\n",
    "        \n",
    "        print(f\"\\n--- Training Enhanced Model {i+1}: {model_name} ---\")\n",
    "        \n",
    "        history = train_advanced_model_for_accuracy(\n",
    "            model=model,\n",
    "            model_name=model_name,\n",
    "            X_train=X_train_scaled,\n",
    "            y_train=y_train_scaled,\n",
    "            X_val=X_val_scaled,\n",
    "            y_val=y_val_scaled,\n",
    "            epochs=ENHANCED_EPOCHS,\n",
    "            batch_size=ENHANCED_BATCH_SIZE\n",
    "        )\n",
    "        \n",
    "        training_results[model_name] = history\n",
    "        \n",
    "        # Estimate R² from validation loss\n",
    "        if history and 'val_loss' in history.history:\n",
    "            best_val_loss = min(history.history['val_loss'])\n",
    "            estimated_r2 = calculate_r2_from_val_loss(best_val_loss, data_variance)\n",
    "            print(f\"  ESTIMATED R² for {model_name}: {estimated_r2:.4f} ({estimated_r2*100:.1f}%)\")\n",
    "            \n",
    "            if estimated_r2 >= 0.85:\n",
    "                print(f\"  🎉 TARGET ACHIEVED! {model_name} reached 85%+ accuracy!\")\n",
    "            elif estimated_r2 >= 0.80:\n",
    "                print(f\"  🔥 EXCELLENT! {model_name} achieved 80%+ accuracy!\")\n",
    "            else:\n",
    "                print(f\"  📈 GOOD PROGRESS! {model_name} achieved {estimated_r2*100:.1f}% accuracy\")\n",
    "        \n",
    "        # Memory management\n",
    "        import gc\n",
    "        gc.collect()\n",
    "    \n",
    "    # Enhanced evaluation\n",
    "    print(\"\\nSTEP 7: ENHANCED MODEL EVALUATION\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    best_model_info = None\n",
    "    best_r2 = 0.0\n",
    "    \n",
    "    print(\"Evaluating all trained models for best performance...\")\n",
    "    \n",
    "    for i, model_info in enumerate(ensemble_models):\n",
    "        model_name = model_info['name']\n",
    "        model = model_info['model']\n",
    "        \n",
    "        try:\n",
    "            # Make predictions\n",
    "            y_pred_scaled = model.predict(X_test_scaled, verbose=0)\n",
    "            y_pred = target_scaler.inverse_transform(y_pred_scaled)\n",
    "            y_true = target_scaler.inverse_transform(y_test_scaled)\n",
    "            \n",
    "            # Calculate comprehensive metrics\n",
    "            mae = mean_absolute_error(y_true.flatten(), y_pred.flatten())\n",
    "            mse = mean_squared_error(y_true.flatten(), y_pred.flatten())\n",
    "            rmse = np.sqrt(mse)\n",
    "            r2 = r2_score(y_true.flatten(), y_pred.flatten())\n",
    "            mape = np.mean(np.abs((y_true.flatten() - y_pred.flatten()) / (y_true.flatten() + 1e-8))) * 100\n",
    "            \n",
    "            print(f\"\\n{model_name} TEST RESULTS:\")\n",
    "            print(f\"  MAE: {mae:.2f}\")\n",
    "            print(f\"  RMSE: {rmse:.2f}\")\n",
    "            print(f\"  MAPE: {mape:.2f}%\")\n",
    "            print(f\"  R²: {r2:.4f} ({r2*100:.1f}%)\")\n",
    "            \n",
    "            if r2 > best_r2:\n",
    "                best_r2 = r2\n",
    "                best_model_info = {\n",
    "                    'name': model_name,\n",
    "                    'model': model,\n",
    "                    'r2': r2,\n",
    "                    'mae': mae,\n",
    "                    'rmse': rmse,\n",
    "                    'mape': mape\n",
    "                }\n",
    "            \n",
    "            # Achievement badges\n",
    "            if r2 >= 0.90:\n",
    "                print(f\"  🏆 OUTSTANDING! 90%+ accuracy achieved!\")\n",
    "            elif r2 >= 0.85:\n",
    "                print(f\"  TARGET ACHIEVED! 85%+ accuracy reached!\")\n",
    "            elif r2 >= 0.80:\n",
    "                print(f\"  EXCELLENT! 80%+ accuracy achieved!\")\n",
    "            else:\n",
    "                print(f\"  Good progress: {r2*100:.1f}% accuracy\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Error evaluating {model_name}: {e}\")\n",
    "    \n",
    "    # Final results\n",
    "    print(f\"\\n\" + \"=\"*70)\n",
    "    print(\"ENHANCED LSTM TRAINING COMPLETED - ACCURACY RESULTS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if best_model_info:\n",
    "        print(f\"\\n BEST MODEL: {best_model_info['name']}\")\n",
    "        print(f\"   R² Accuracy: {best_model_info['r2']:.4f} ({best_model_info['r2']*100:.1f}%)\")\n",
    "        print(f\"   MAE: {best_model_info['mae']:.2f}\")\n",
    "        print(f\"   RMSE: {best_model_info['rmse']:.2f}\")\n",
    "        print(f\"   MAPE: {best_model_info['mape']:.2f}%\")\n",
    "        \n",
    "        if best_model_info['r2'] >= 0.85:\n",
    "            print(f\"\\n TARGET OF 85%+ ACCURACY ACHIEVED!\")\n",
    "            print(f\"Your model has reached {best_model_info['r2']*100:.1f}% R² accuracy!\")\n",
    "        elif best_model_info['r2'] >= 0.80:\n",
    "            print(f\"\\n 80%+ accuracy achieved!\")\n",
    "            print(f\"Very close to 85% target - try extended training for final push!\")\n",
    "        else:\n",
    "            print(f\"Progress report! {best_model_info['r2']*100:.1f}% accuracy achieved!\")\n",
    "            print(f\"Recommendations for reaching 85%:\")\n",
    "            print(f\"  • Increase training epochs to 200+\")\n",
    "            print(f\"  • Add more diverse features\")\n",
    "            print(f\"  • Try ensemble averaging\")\n",
    "    \n",
    "    print(f\"\\nOptimizations Applied:\")\n",
    "    print(f\"  ✓ Extended training: {ENHANCED_EPOCHS} epochs\")\n",
    "    print(f\"  ✓ Enhanced features: {len(available_features)} variables\")\n",
    "    print(f\"  ✓ Weekly sequence patterns: {SEQUENCE_LENGTH} hours\")\n",
    "    print(f\"  ✓ Advanced architectures: Deep, Hybrid, Transformer models\")\n",
    "    print(f\"  ✓ Sophisticated callbacks: Cyclical LR, extended patience\")\n",
    "    \n",
    "    return best_model_info, training_results\n",
    "\n",
    "# Execute the optimized pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    best_model, results = main_optimized_for_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a26ec71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyAgrum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
